{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnADoO33MO1E",
        "outputId": "0b4bfd79-b234-4a70-a0a1-b4159e1293e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-19 08:07:19--  https://archive.ics.uci.edu/static/public/352/online+retail.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘online+retail.zip’\n",
            "\n",
            "online+retail.zip       [  <=>               ]  22.62M  88.0MB/s    in 0.3s    \n",
            "\n",
            "2024-11-19 08:07:19 (88.0 MB/s) - ‘online+retail.zip’ saved [23715478]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/352/online+retail.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip online+retail.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvhX8JvwM0Eg",
        "outputId": "85fee0fb-b845-4f61-c3da-1aef08061127"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  online+retail.zip\n",
            " extracting: Online Retail.xlsx      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel(\"Online Retail.xlsx\")\n",
        "data.to_csv(\"Online Retail.csv\", index=False)"
      ],
      "metadata": {
        "id": "8Z3Ruv-zaCD5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr, sum, avg, count, max, min, year, month, dayofmonth, desc\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Data Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class RetailDataAnalysis:\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Initialize Spark session and load retail data\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the UCI Online Retail CSV file\n",
        "        \"\"\"\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"UCI Online Retail Comprehensive Analysis\") \\\n",
        "            .config(\"spark.sql.pandas.outputConvertMode\", \"arrow\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        # Set visualization style\n",
        "        plt.style.use(\"seaborn-v0_8\")\n",
        "        sns.set_palette(\"deep\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self.df = self.load_retail_data(file_path)\n",
        "\n",
        "    def load_retail_data(self, file_path):\n",
        "        \"\"\"\n",
        "        Load and clean UCI Online Retail dataset\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: Processed retail dataset\n",
        "        \"\"\"\n",
        "        schema = StructType([\n",
        "            StructField(\"InvoiceNo\", StringType(), True),\n",
        "            StructField(\"StockCode\", StringType(), True),\n",
        "            StructField(\"Description\", StringType(), True),\n",
        "            StructField(\"Quantity\", IntegerType(), True),\n",
        "            StructField(\"InvoiceDate\", TimestampType(), True),\n",
        "            StructField(\"UnitPrice\", DoubleType(), True),\n",
        "            StructField(\"CustomerID\", StringType(), True),\n",
        "            StructField(\"Country\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        df = self.spark.read \\\n",
        "            .format(\"csv\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .schema(schema) \\\n",
        "            .load(file_path)\n",
        "\n",
        "        # Data cleaning and preprocessing\n",
        "        df = df.filter(col(\"Quantity\") > 0) \\\n",
        "               .filter(col(\"UnitPrice\") > 0) \\\n",
        "               .na.drop(subset=[\"CustomerID\"])\n",
        "\n",
        "        df = df.withColumn(\"TotalSales\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def country_sales_visualization(self):\n",
        "        \"\"\"\n",
        "        Visualize sales distribution across countries\n",
        "        \"\"\"\n",
        "        country_sales = self.df.groupBy(\"Country\") \\\n",
        "            .agg(\n",
        "                sum(\"TotalSales\").alias(\"TotalCountrySales\"),\n",
        "                count(\"InvoiceNo\").alias(\"TransactionCount\")\n",
        "            ) \\\n",
        "            .orderBy(col(\"TotalCountrySales\").desc()) \\\n",
        "            .toPandas()\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.subplot(121)\n",
        "        sns.barplot(x='Country', y='TotalCountrySales', data=country_sales.head(10))\n",
        "        plt.title('Top 10 Countries by Total Sales')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(122)\n",
        "        sns.barplot(x='Country', y='TransactionCount', data=country_sales.head(10))\n",
        "        plt.title('Top 10 Countries by Transaction Volume')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig('country_sales_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def time_series_analysis(self):\n",
        "        \"\"\"\n",
        "        Perform and visualize time series analysis\n",
        "        \"\"\"\n",
        "        from pyspark.sql.functions import to_date\n",
        "\n",
        "        # Monthly sales trend\n",
        "        monthly_sales = self.df.groupBy(\n",
        "            year(\"InvoiceDate\").alias(\"Year\"),\n",
        "            month(\"InvoiceDate\").alias(\"Month\")\n",
        "        ).agg(\n",
        "            sum(\"TotalSales\").alias(\"MonthlySales\")\n",
        "        ).orderBy(\"Year\", \"Month\").toPandas()\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.subplot(121)\n",
        "        sns.lineplot(x='Month', y='MonthlySales', hue='Year', data=monthly_sales)\n",
        "        plt.title('Monthly Sales Trend')\n",
        "        plt.xlabel('Month')\n",
        "        plt.ylabel('Total Sales')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        # Daily sales trend\n",
        "        daily_sales = self.df.groupBy(to_date(col(\"InvoiceDate\")).alias(\"Date\")) \\\n",
        "            .agg(sum(\"TotalSales\").alias(\"DailySales\")) \\\n",
        "            .orderBy(\"Date\").toPandas()\n",
        "\n",
        "        sns.lineplot(x='Date', y='DailySales', data=daily_sales)\n",
        "        plt.title('Daily Sales Trend')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def product_analysis(self):\n",
        "        \"\"\"\n",
        "        Analyze and visualize product sales\n",
        "        \"\"\"\n",
        "        product_sales = self.df.groupBy(\"StockCode\", \"Description\") \\\n",
        "            .agg(\n",
        "                sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "                sum(\"TotalSales\").alias(\"TotalProductSales\")\n",
        "            ) \\\n",
        "            .orderBy(col(\"TotalProductSales\").desc()) \\\n",
        "            .limit(10).toPandas()\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.subplot(121)\n",
        "        sns.barplot(x='Description', y='TotalQuantity', data=product_sales)\n",
        "        plt.title('Top 10 Products by Quantity Sold')\n",
        "        plt.xticks(rotation=90)\n",
        "\n",
        "        plt.subplot(122)\n",
        "        sns.barplot(x='Description', y='TotalProductSales', data=product_sales)\n",
        "        plt.title('Top 10 Products by Total Sales')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig('product_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def customer_segmentation(self):\n",
        "        \"\"\"\n",
        "        Perform customer segmentation with visualization\n",
        "        \"\"\"\n",
        "        # Prepare customer features for clustering\n",
        "        customer_features = self.df.groupBy(\"CustomerID\") \\\n",
        "            .agg(\n",
        "                sum(\"TotalSales\").alias(\"TotalSpent\"),\n",
        "                count(\"InvoiceNo\").alias(\"TransactionCount\")\n",
        "            ).toPandas()\n",
        "\n",
        "        # Normalize features\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        features = scaler.fit_transform(customer_features[['TotalSpent', 'TransactionCount']])\n",
        "\n",
        "        # K-means clustering\n",
        "        from sklearn.cluster import KMeans\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        customer_features['Segment'] = kmeans.fit_predict(features)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.scatterplot(\n",
        "            x='TotalSpent',\n",
        "            y='TransactionCount',\n",
        "            hue='Segment',\n",
        "            data=customer_features,\n",
        "            palette='viridis'\n",
        "        )\n",
        "        plt.title('Customer Segmentation')\n",
        "        plt.xlabel('Total Spending')\n",
        "        plt.ylabel('Number of Transactions')\n",
        "        plt.savefig('customer_segmentation.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive analysis with visualizations\n",
        "        \"\"\"\n",
        "        self.country_sales_visualization()\n",
        "        self.time_series_analysis()\n",
        "        self.product_analysis()\n",
        "        self.customer_segmentation()\n",
        "\n",
        "        print(\"Comprehensive analysis complete. Generated visualizations:\")\n",
        "        print(\"1. country_sales_analysis.png\")\n",
        "        print(\"2. time_series_analysis.png\")\n",
        "        print(\"3. product_analysis.png\")\n",
        "        print(\"4. customer_segmentation.png\")\n",
        "\n",
        "def main():\n",
        "    # Replace with your actual file path\n",
        "    file_path = \"Online Retail.csv\"\n",
        "\n",
        "    # Create analysis instance\n",
        "    analysis = RetailDataAnalysis(file_path)\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    analysis.generate_comprehensive_report()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzC_hNVgWpd0",
        "outputId": "79f342b4-9e56-41b5-c34e-89a02fd7a3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensive analysis complete. Generated visualizations:\n",
            "1. country_sales_analysis.png\n",
            "2. time_series_analysis.png\n",
            "3. product_analysis.png\n",
            "4. customer_segmentation.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, expr, sum, avg, count, max, min, year, month, dayofmonth, desc, udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType, ArrayType\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
        "from pyspark.ml.clustering import KMeans, GaussianMixture\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, ClusteringEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Data Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import zscore\n",
        "\n",
        "class AdvancedRetailDataAnalysis:\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Initialize Spark session with advanced configurations\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the UCI Online Retail CSV file\n",
        "        \"\"\"\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"Advanced UCI Online Retail Analysis\") \\\n",
        "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "            .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        # Advanced visualization setup\n",
        "        plt.style.use(\"seaborn-v0_8-dark\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self.df = self.load_and_preprocess_data(file_path)\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path):\n",
        "        \"\"\"\n",
        "        Advanced data loading and preprocessing\n",
        "        \"\"\"\n",
        "        schema = StructType([\n",
        "            StructField(\"InvoiceNo\", StringType(), True),\n",
        "            StructField(\"StockCode\", StringType(), True),\n",
        "            StructField(\"Description\", StringType(), True),\n",
        "            StructField(\"Quantity\", IntegerType(), True),\n",
        "            StructField(\"InvoiceDate\", TimestampType(), True),\n",
        "            StructField(\"UnitPrice\", DoubleType(), True),\n",
        "            StructField(\"CustomerID\", StringType(), True),\n",
        "            StructField(\"Country\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        df = self.spark.read \\\n",
        "            .format(\"csv\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .schema(schema) \\\n",
        "            .load(file_path)\n",
        "\n",
        "        # Advanced cleaning and feature engineering\n",
        "        df = df.filter(col(\"Quantity\") > 0) \\\n",
        "               .filter(col(\"UnitPrice\") > 0) \\\n",
        "               .na.drop(subset=[\"CustomerID\"])\n",
        "\n",
        "        df = df.withColumn(\"TotalSales\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "               .withColumn(\"Year\", year(\"InvoiceDate\")) \\\n",
        "               .withColumn(\"Month\", month(\"InvoiceDate\")) \\\n",
        "               .withColumn(\"DayOfWeek\", dayofmonth(\"InvoiceDate\"))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def advanced_customer_segmentation(self):\n",
        "        \"\"\"\n",
        "        Advanced customer segmentation using multiple techniques\n",
        "        \"\"\"\n",
        "        # Prepare customer features\n",
        "        customer_features_df = self.df.groupBy(\"CustomerID\") \\\n",
        "            .agg(\n",
        "                sum(\"TotalSales\").alias(\"TotalSpent\"),\n",
        "                count(\"InvoiceNo\").alias(\"TransactionCount\"),\n",
        "                avg(\"UnitPrice\").alias(\"AvgUnitPrice\"),\n",
        "                max(\"TotalSales\").alias(\"MaxSinglePurchase\")\n",
        "            )\n",
        "\n",
        "        # Convert to Pandas for advanced preprocessing\n",
        "        customer_features_pd = customer_features_df.toPandas()\n",
        "\n",
        "        # Normalize features using z-score\n",
        "        features_scaled = customer_features_pd[['TotalSpent', 'TransactionCount', 'AvgUnitPrice', 'MaxSinglePurchase']].apply(zscore)\n",
        "\n",
        "        # Multiple clustering techniques\n",
        "        # 1. K-means Clustering\n",
        "        from sklearn.cluster import KMeans\n",
        "        kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "        kmeans_labels = kmeans.fit_predict(features_scaled)\n",
        "\n",
        "        # 2. Gaussian Mixture Model\n",
        "        from sklearn.mixture import GaussianMixture\n",
        "        gmm = GaussianMixture(n_components=4, random_state=42)\n",
        "        gmm_labels = gmm.fit_predict(features_scaled)\n",
        "\n",
        "        # Add cluster labels to DataFrame\n",
        "        customer_features_pd['KMeans_Cluster'] = kmeans_labels\n",
        "        customer_features_pd['GMM_Cluster'] = gmm_labels\n",
        "\n",
        "        # Interactive 3D scatter plot with Plotly\n",
        "        fig = px.scatter_3d(\n",
        "            customer_features_pd,\n",
        "            x='TotalSpent',\n",
        "            y='TransactionCount',\n",
        "            z='AvgUnitPrice',\n",
        "            color='KMeans_Cluster',\n",
        "            title='Advanced Customer Segmentation',\n",
        "            labels={'KMeans_Cluster': 'Customer Segment'}\n",
        "        )\n",
        "        fig.write_html('advanced_customer_segmentation.html')\n",
        "\n",
        "    def predictive_product_analysis(self):\n",
        "        \"\"\"\n",
        "        Advanced predictive analysis for product sales\n",
        "        \"\"\"\n",
        "        # Prepare product features for prediction\n",
        "        product_features = self.df.groupBy(\"StockCode\", \"Description\") \\\n",
        "            .agg(\n",
        "                sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "                sum(\"TotalSales\").alias(\"TotalSales\"),\n",
        "                avg(\"UnitPrice\").alias(\"AvgPrice\")\n",
        "            )\n",
        "\n",
        "        # Convert to Vector Assembler format\n",
        "        assembler = VectorAssembler(inputCols=[\"TotalQuantity\", \"AvgPrice\"], outputCol=\"features\")\n",
        "        product_data = assembler.transform(product_features)\n",
        "\n",
        "        # Split data\n",
        "        (train_data, test_data) = product_data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "        # Multiple regression techniques\n",
        "        # 1. Linear Regression\n",
        "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"TotalSales\")\n",
        "        lr_model = lr.fit(train_data)\n",
        "\n",
        "        # 2. Random Forest Regression\n",
        "        rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"TotalSales\")\n",
        "        rf_model = rf.fit(train_data)\n",
        "\n",
        "        # Evaluate models\n",
        "        lr_predictions = lr_model.transform(test_data)\n",
        "        rf_predictions = rf_model.transform(test_data)\n",
        "\n",
        "        evaluator = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\")\n",
        "\n",
        "        print(\"Linear Regression R² Score:\", evaluator.evaluate(lr_predictions))\n",
        "        print(\"Random Forest R² Score:\", evaluator.evaluate(rf_predictions))\n",
        "\n",
        "        # Visualize feature importance for Random Forest\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'feature': ['TotalQuantity', 'AvgPrice'], # Use feature names from VectorAssembler\n",
        "            'importance': rf_model.featureImportances.toArray() # Convert SparseVector to NumPy array\n",
        "        })\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='feature', y='importance', data=feature_importances)\n",
        "        plt.title('Feature Importance in Product Sales Prediction')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('product_sales_feature_importance.png')\n",
        "        plt.close()\n",
        "\n",
        "    def temporal_pattern_discovery(self):\n",
        "        \"\"\"\n",
        "        Advanced temporal pattern analysis using machine learning\n",
        "        \"\"\"\n",
        "        # Time-based features\n",
        "        time_features = self.df.groupBy(\"Year\", \"Month\") \\\n",
        "            .agg(\n",
        "                sum(\"TotalSales\").alias(\"MonthlySales\"),\n",
        "                count(\"InvoiceNo\").alias(\"TransactionCount\")\n",
        "            ) \\\n",
        "            .orderBy(\"Year\", \"Month\")\n",
        "\n",
        "        # Convert to Pandas for advanced time series analysis\n",
        "        time_df = time_features.toPandas()\n",
        "\n",
        "        # Seasonal decomposition\n",
        "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "        # Assuming we want to decompose monthly sales\n",
        "        decomposition = seasonal_decompose(time_df['MonthlySales'], period=2)\n",
        "\n",
        "        # Create subplots for decomposition\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.subplot(411)\n",
        "        plt.plot(decomposition.observed)\n",
        "        plt.title('Observed')\n",
        "        plt.subplot(412)\n",
        "        plt.plot(decomposition.trend)\n",
        "        plt.title('Trend')\n",
        "        plt.subplot(413)\n",
        "        plt.plot(decomposition.seasonal)\n",
        "        plt.title('Seasonal')\n",
        "        plt.subplot(414)\n",
        "        plt.plot(decomposition.resid)\n",
        "        plt.title('Residual')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sales_decomposition.png')\n",
        "        plt.close()\n",
        "\n",
        "    def generate_advanced_report(self):\n",
        "        \"\"\"\n",
        "        Generate comprehensive advanced analysis\n",
        "        \"\"\"\n",
        "        self.advanced_customer_segmentation()\n",
        "        self.predictive_product_analysis()\n",
        "        self.temporal_pattern_discovery()\n",
        "\n",
        "        print(\"Advanced analysis complete with:\")\n",
        "        print(\"1. Interactive 3D Customer Segmentation (advanced_customer_segmentation.html)\")\n",
        "        print(\"2. Product Sales Feature Importance (product_sales_feature_importance.png)\")\n",
        "        print(\"3. Sales Temporal Decomposition (sales_decomposition.png)\")\n",
        "\n",
        "def main():\n",
        "    # Replace with your actual file path\n",
        "    file_path = \"Online Retail.csv\"\n",
        "\n",
        "    # Create advanced analysis instance\n",
        "    analysis = AdvancedRetailDataAnalysis(file_path)\n",
        "\n",
        "    # Generate advanced report\n",
        "    analysis.generate_advanced_report()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYrm1hGWsup",
        "outputId": "0b4a16e8-1bb6-49ff-dd56-3a63d135fd4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R² Score: 3577.530374378818\n",
            "Random Forest R² Score: 2711.07845662271\n",
            "Advanced analysis complete with:\n",
            "1. Interactive 3D Customer Segmentation (advanced_customer_segmentation.html)\n",
            "2. Product Sales Feature Importance (product_sales_feature_importance.png)\n",
            "3. Sales Temporal Decomposition (sales_decomposition.png)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required installations for Google Colab environment\n",
        "print(\"Installing required packages...\")\n",
        "# !pip install pyspark==3.4.1 sparknlp seaborn matplotlib pandas\n",
        "print(\"Package installation complete.\")\n",
        "\n",
        "# Core imports\n",
        "print(\"\\nImporting required libraries...\")\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, udf, lower, regexp_replace, explode\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    ArrayType, StringType, StructType, StructField,\n",
        "    IntegerType, TimestampType, DoubleType\n",
        ")\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer as MLTokenizer\n",
        "from pyspark.ml.clustering import KMeans as MLKMeans\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Spark NLP Imports\n",
        "import sparknlp\n",
        "from sparknlp.base import DocumentAssembler, LightPipeline\n",
        "from sparknlp.annotator import (\n",
        "    Tokenizer, Normalizer, StopWordsCleaner,\n",
        "    WordEmbeddingsModel, SentenceDetector, NerDLModel\n",
        ")\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "print(\"Library imports complete.\")\n",
        "\n",
        "class SparkNLPRetailAnalysis:\n",
        "    \"\"\"\n",
        "    A comprehensive class for analyzing retail data using Spark NLP with detailed progress tracking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"Initialize the SparkNLPRetailAnalysis with progress tracking.\"\"\"\n",
        "        print(\"\\nInitializing SparkNLPRetailAnalysis...\")\n",
        "        print(\"Starting Spark session with NLP support...\")\n",
        "        self.spark = sparknlp.start()\n",
        "        print(\"Spark session started successfully.\")\n",
        "\n",
        "        print(\"\\nConfiguring Spark settings for optimal performance...\")\n",
        "        self._configure_spark_session()\n",
        "        print(\"Spark configuration complete.\")\n",
        "\n",
        "        print(\"\\nLoading and preprocessing retail data...\")\n",
        "        self.df = self.load_retail_data(file_path)\n",
        "        print(f\"Data loaded successfully. Row count: {self.df.count()}\")\n",
        "\n",
        "        print(\"\\nCreating NLP pipeline...\")\n",
        "        self.nlp_pipeline = self.create_nlp_pipeline()\n",
        "        print(\"NLP pipeline created successfully.\")\n",
        "\n",
        "    def _configure_spark_session(self):\n",
        "        \"\"\"Configure Spark session with optimization settings.\"\"\"\n",
        "        configs = {\n",
        "            \"spark.sql.shuffle.partitions\": \"200\",\n",
        "            \"spark.default.parallelism\": \"100\",\n",
        "            # \"spark.executor.memory\": \"2g\",\n",
        "            # \"spark.driver.memory\": \"2g\"\n",
        "        }\n",
        "        for key, value in configs.items():\n",
        "            print(f\"Setting {key} to {value}\")\n",
        "            self.spark.conf.set(key, value)\n",
        "\n",
        "    def load_retail_data(self, file_path: str):\n",
        "        \"\"\"Load and preprocess the retail dataset with progress tracking.\"\"\"\n",
        "        print(\"\\nDefining schema for retail data...\")\n",
        "        schema = StructType([\n",
        "            StructField(\"InvoiceNo\", StringType(), True),\n",
        "            StructField(\"StockCode\", StringType(), True),\n",
        "            StructField(\"Description\", StringType(), True),\n",
        "            StructField(\"Quantity\", IntegerType(), True),\n",
        "            StructField(\"InvoiceDate\", TimestampType(), True),\n",
        "            StructField(\"UnitPrice\", DoubleType(), True),\n",
        "            StructField(\"CustomerID\", StringType(), True),\n",
        "            StructField(\"Country\", StringType(), True)\n",
        "        ])\n",
        "        print(\"Schema defined successfully.\")\n",
        "\n",
        "        print(\"\\nLoading data from CSV...\")\n",
        "        df = (\n",
        "            self.spark.read.format(\"csv\")\n",
        "            .option(\"header\", \"true\")\n",
        "            .schema(schema)\n",
        "            .load(file_path)\n",
        "        )\n",
        "        print(f\"Initial data load complete. Row count: {df.count()}\")\n",
        "\n",
        "        print(\"\\nFiltering for non-null descriptions...\")\n",
        "        df = df.filter(col(\"Description\").isNotNull())\n",
        "        print(f\"Filtered data row count: {df.count()}\")\n",
        "\n",
        "        print(\"\\nCleaning description text...\")\n",
        "        df = df.withColumn(\n",
        "            \"CleanDescription\",\n",
        "            lower(regexp_replace(col(\"Description\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "        )\n",
        "        print(\"Text cleaning complete.\")\n",
        "\n",
        "        print(\"\\nRepartitioning dataset...\")\n",
        "        df = df.repartition(50)\n",
        "        print(\"Data preparation complete.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_nlp_pipeline(self):\n",
        "        \"\"\"Create advanced NLP processing pipeline with progress tracking.\"\"\"\n",
        "        print(\"\\nInitializing NLP pipeline components...\")\n",
        "\n",
        "        print(\"Setting up DocumentAssembler...\")\n",
        "        documentAssembler = DocumentAssembler() \\\n",
        "            .setInputCol(\"CleanDescription\") \\\n",
        "            .setOutputCol(\"document\")\n",
        "\n",
        "        print(\"Setting up SentenceDetector...\")\n",
        "        sentenceDetector = SentenceDetector() \\\n",
        "            .setInputCols([\"document\"]) \\\n",
        "            .setOutputCol(\"sentence\")\n",
        "\n",
        "        print(\"Setting up Tokenizer...\")\n",
        "        tokenizer = Tokenizer() \\\n",
        "            .setInputCols([\"sentence\"]) \\\n",
        "            .setOutputCol(\"token\")\n",
        "\n",
        "        print(\"Setting up Normalizer...\")\n",
        "        normalizer = Normalizer() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"normalized\")\n",
        "\n",
        "        print(\"Setting up StopWordsCleaner...\")\n",
        "        stopWordsCleaner = StopWordsCleaner() \\\n",
        "            .setInputCols([\"normalized\"]) \\\n",
        "            .setOutputCol(\"cleanTokens\")\n",
        "\n",
        "        print(\"Loading word embeddings model...\")\n",
        "        embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
        "            .setInputCols([\"document\", \"token\"]) \\\n",
        "            .setOutputCol(\"embeddings\")\n",
        "        print(\"Word embeddings loaded successfully.\")\n",
        "\n",
        "        print(\"Loading NER model...\")\n",
        "        nerModel = NerDLModel.pretrained(\"ner_dl\", \"en\") \\\n",
        "            .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
        "            .setOutputCol(\"ner\")\n",
        "        print(\"NER model loaded successfully.\")\n",
        "\n",
        "        print(\"Assembling complete pipeline...\")\n",
        "        return sparknlp.base.Pipeline(stages=[\n",
        "            documentAssembler,\n",
        "            sentenceDetector,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopWordsCleaner,\n",
        "            embeddings,\n",
        "            nerModel\n",
        "        ])\n",
        "\n",
        "    def perform_text_analysis(self):\n",
        "        \"\"\"Perform comprehensive text analysis with detailed progress tracking.\"\"\"\n",
        "        print(\"\\nStarting text analysis process...\")\n",
        "\n",
        "        print(\"Fitting NLP pipeline to data...\")\n",
        "        nlp_model = self.nlp_pipeline.fit(self.df)\n",
        "        print(\"Pipeline fitted successfully.\")\n",
        "\n",
        "        print(\"Transforming data through NLP pipeline...\")\n",
        "        processed_df = nlp_model.transform(self.df)\n",
        "        print(\"NLP transformation complete.\")\n",
        "\n",
        "        print(\"\\nDefining entity extraction UDF...\")\n",
        "        def extract_entities(ner_annotations):\n",
        "            if not ner_annotations:\n",
        "                return []\n",
        "            return [\n",
        "                (entity.result, entity.metadata['entity'])\n",
        "                for entity in ner_annotations\n",
        "                if 'entity' in entity.metadata\n",
        "            ]\n",
        "\n",
        "        extract_entities_udf = udf(\n",
        "            extract_entities,\n",
        "            ArrayType(\n",
        "                StructType([\n",
        "                    StructField(\"text\", StringType(), True),\n",
        "                    StructField(\"entity_type\", StringType(), True)\n",
        "                ])\n",
        "            )\n",
        "        )\n",
        "        print(\"Entity extraction UDF defined.\")\n",
        "\n",
        "        print(\"\\nExtracting named entities...\")\n",
        "        entity_df = processed_df.withColumn(\n",
        "            \"extracted_entities\",\n",
        "            extract_entities_udf(col(\"ner\"))\n",
        "        )\n",
        "        print(\"Entity extraction complete.\")\n",
        "\n",
        "        print(\"\\nAnalyzing entity distribution...\")\n",
        "        # entity_analysis = (\n",
        "        #     entity_df.select(explode(\"extracted_entities\").alias(\"entity\"))\n",
        "        #     .groupBy(\"entity.entity_type\")\n",
        "        #     .count()\n",
        "        #     .orderBy(col(\"count\").desc())\n",
        "        #     .toPandas()\n",
        "        # )\n",
        "        exploded_entities = entity_df.select(explode(\"extracted_entities\").alias(\"entity\")).cache()\n",
        "        spark_entity_analysis = (\n",
        "            exploded_entities\n",
        "            .select(col(\"entity.entity_type\").alias(\"entity_type\"))\n",
        "            .groupBy(\"entity_type\")\n",
        "            .agg(expr(\"count(1)\").alias(\"count\"))\n",
        "            .orderBy(col(\"count\").desc())\n",
        "        )\n",
        "\n",
        "        # Convert to Pandas after reducing the data size\n",
        "        entity_analysis = spark_entity_analysis.limit(50).toPandas()\n",
        "\n",
        "        print(\"Entity distribution analysis complete.\")\n",
        "\n",
        "        print(\"\\nGenerating entity distribution visualization...\")\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(x='entity_type', y='count', data=entity_analysis)\n",
        "        plt.title('Distribution of Named Entities in Product Descriptions')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('named_entities_distribution.png')\n",
        "        plt.close()\n",
        "        print(\"Entity distribution visualization saved.\")\n",
        "\n",
        "        print(\"\\nAnalyzing word frequencies...\")\n",
        "        word_freq = (\n",
        "            processed_df.select(explode(\"cleanTokens.result\").alias(\"word\"))\n",
        "            .groupBy(\"word\")\n",
        "            .count()\n",
        "            .orderBy(col(\"count\").desc())\n",
        "            .limit(50)\n",
        "            .toPandas()\n",
        "        )\n",
        "        print(\"Word frequency analysis complete.\")\n",
        "\n",
        "        print(\"\\nGenerating word frequency visualization...\")\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        sns.barplot(x='word', y='count', data=word_freq.head(20))\n",
        "        plt.title('Top 20 Most Frequent Words in Product Descriptions')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('word_frequency.png')\n",
        "        plt.close()\n",
        "        print(\"Word frequency visualization saved.\")\n",
        "\n",
        "        print(\"\\nSetting up text clustering pipeline...\")\n",
        "        tokenizer = MLTokenizer(inputCol=\"CleanDescription\", outputCol=\"words\")\n",
        "        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
        "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "        kmeans = MLKMeans(k=5, featuresCol=\"features\")\n",
        "\n",
        "        clustering_pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, kmeans])\n",
        "        print(\"Clustering pipeline created.\")\n",
        "\n",
        "        print(\"\\nPerforming text clustering...\")\n",
        "        model = clustering_pipeline.fit(self.df)\n",
        "        clustered_df = model.transform(self.df)\n",
        "        print(\"Text clustering complete.\")\n",
        "\n",
        "        print(\"\\nText Analysis Process Complete!\")\n",
        "        print(\"Generated outputs:\")\n",
        "        print(\"1. named_entities_distribution.png\")\n",
        "        print(\"2. word_frequency.png\")\n",
        "\n",
        "        return clustered_df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with progress tracking.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting Retail Analysis Process\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Replace with your CSV file path\n",
        "    file_path = \"/content/Online Retail.csv\"\n",
        "\n",
        "    print(\"\\nInitializing analysis...\")\n",
        "    nlp_analysis = SparkNLPRetailAnalysis(file_path)\n",
        "\n",
        "    print(\"\\nStarting text analysis...\")\n",
        "    nlp_analysis.perform_text_analysis()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Analysis Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3OBQGTWXqfR",
        "outputId": "687fe624-a1c0-469d-a7c1-583a2314f3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "Package installation complete.\n",
            "\n",
            "Importing required libraries...\n",
            "Library imports complete.\n",
            "\n",
            "==================================================\n",
            "Starting Retail Analysis Process\n",
            "==================================================\n",
            "\n",
            "Initializing analysis...\n",
            "\n",
            "Initializing SparkNLPRetailAnalysis...\n",
            "Starting Spark session with NLP support...\n",
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark session started successfully.\n",
            "\n",
            "Configuring Spark settings for optimal performance...\n",
            "Setting spark.sql.shuffle.partitions to 200\n",
            "Setting spark.default.parallelism to 100\n",
            "Spark configuration complete.\n",
            "\n",
            "Loading and preprocessing retail data...\n",
            "\n",
            "Defining schema for retail data...\n",
            "Schema defined successfully.\n",
            "\n",
            "Loading data from CSV...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "class RetailDataVisualizer:\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Initialize visualizer with retail dataset\n",
        "        \"\"\"\n",
        "        # Read the dataset\n",
        "        self.df = pd.read_csv(file_path, parse_dates=['InvoiceDate'], encoding='unicode_escape')\n",
        "\n",
        "        # Clean and preprocess data\n",
        "        self.preprocess_data()\n",
        "\n",
        "        # Set up color palette\n",
        "        self.color_palette = px.colors.sequential.Viridis\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Clean and prepare the dataset for visualization\n",
        "        \"\"\"\n",
        "        # Remove negative quantities and zero prices\n",
        "        self.df = self.df[(self.df['Quantity'] > 0) & (self.df['UnitPrice'] > 0)]\n",
        "\n",
        "        # Calculate total sales\n",
        "        self.df['TotalSales'] = self.df['Quantity'] * self.df['UnitPrice']\n",
        "\n",
        "        # Convert InvoiceDate to datetime if not already\n",
        "        self.df['InvoiceDate'] = pd.to_datetime(self.df['InvoiceDate'])\n",
        "\n",
        "        # Extract additional time features\n",
        "        self.df['Year'] = self.df['InvoiceDate'].dt.year\n",
        "        self.df['Month'] = self.df['InvoiceDate'].dt.month\n",
        "        self.df['DayOfWeek'] = self.df['InvoiceDate'].dt.day_name()\n",
        "\n",
        "    def generate_advanced_visualizations(self):\n",
        "        \"\"\"\n",
        "        Create comprehensive visualizations from the retail dataset\n",
        "        \"\"\"\n",
        "        # 1. Geographical Sales Distribution\n",
        "        country_sales = self.df.groupby('Country')['TotalSales'].sum().reset_index()\n",
        "\n",
        "        # Plotly Geographical Heatmap\n",
        "        fig_geo = px.scatter_geo(\n",
        "            country_sales,\n",
        "            locations='Country',\n",
        "            locationmode='country names',\n",
        "            color='TotalSales',\n",
        "            size='TotalSales',\n",
        "            hover_name='Country',\n",
        "            color_continuous_scale=self.color_palette,\n",
        "            projection='natural earth',\n",
        "            title='Global Sales Distribution by Country'\n",
        "        )\n",
        "        fig_geo.write_html('global_sales_distribution.html')\n",
        "\n",
        "        # 2. Time Series Analysis of Monthly Sales\n",
        "        monthly_sales = self.df.groupby([self.df['InvoiceDate'].dt.to_period('M')])['TotalSales'].sum().reset_index()\n",
        "        monthly_sales['InvoiceDate'] = monthly_sales['InvoiceDate'].dt.to_timestamp()\n",
        "\n",
        "        # Interactive Time Series Plot\n",
        "        fig_time = go.Figure()\n",
        "        fig_time.add_trace(go.Scatter(\n",
        "            x=monthly_sales['InvoiceDate'],\n",
        "            y=monthly_sales['TotalSales'],\n",
        "            mode='lines+markers',\n",
        "            name='Monthly Sales',\n",
        "            line=dict(color=self.color_palette[3], width=3)\n",
        "        ))\n",
        "\n",
        "        fig_time.update_layout(\n",
        "            title='Monthly Sales Trend',\n",
        "            xaxis_title='Date',\n",
        "            yaxis_title='Total Sales',\n",
        "            hovermode='x unified'\n",
        "        )\n",
        "        fig_time.write_html('monthly_sales_trend.html')\n",
        "\n",
        "        # 3. Product Category Analysis\n",
        "        # Use Description as a proxy for product category\n",
        "        product_sales = self.df.groupby('Description')['TotalSales'].agg(['sum', 'count']).reset_index()\n",
        "        product_sales = product_sales.sort_values('sum', ascending=False).head(15)\n",
        "\n",
        "        # Plotly Bar Chart\n",
        "        fig_products = px.bar(\n",
        "            product_sales,\n",
        "            x='Description',\n",
        "            y='sum',\n",
        "            color='count',\n",
        "            title='Top 15 Products by Total Sales',\n",
        "            labels={'sum': 'Total Sales', 'count': 'Transaction Count'},\n",
        "            color_continuous_scale=self.color_palette\n",
        "        )\n",
        "        fig_products.update_xaxes(tickangle=45)\n",
        "        fig_products.write_html('top_products_sales.html')\n",
        "\n",
        "        # 4. Customer Segmentation Visualization\n",
        "        # Recency, Frequency, Monetary (RFM) Analysis\n",
        "        from datetime import datetime\n",
        "\n",
        "        # Calculate RFM metrics\n",
        "        rfm_data = self.df.groupby('CustomerID').agg({\n",
        "            'InvoiceDate': lambda x: (datetime.now() - x.max()).days,  # Recency\n",
        "            'InvoiceNo': 'count',  # Frequency\n",
        "            'TotalSales': 'sum'  # Monetary\n",
        "        }).reset_index()\n",
        "\n",
        "        rfm_data.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "        # 3D Scatter for Customer Segmentation\n",
        "        fig_customers = px.scatter_3d(\n",
        "            rfm_data,\n",
        "            x='Recency',\n",
        "            y='Frequency',\n",
        "            z='Monetary',\n",
        "            color='Monetary',\n",
        "            size='Monetary',\n",
        "            hover_data=['CustomerID'],\n",
        "            title='Customer Segmentation: Recency, Frequency, Monetary',\n",
        "            color_continuous_scale=self.color_palette\n",
        "        )\n",
        "        fig_customers.write_html('customer_segmentation_3d.html')\n",
        "\n",
        "        # 5. Correlation Heatmap of Sales Metrics\n",
        "        # Create correlation matrix of derived features\n",
        "        correlation_df = rfm_data[['Recency', 'Frequency', 'Monetary']].corr()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            correlation_df,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            square=True\n",
        "        )\n",
        "        plt.title('Correlation of Customer Metrics')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('customer_metrics_correlation.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Advanced Retail Data Visualizations Generated:\")\n",
        "        print(\"1. global_sales_distribution.html\")\n",
        "        print(\"2. monthly_sales_trend.html\")\n",
        "        print(\"3. top_products_sales.html\")\n",
        "        print(\"4. customer_segmentation_3d.html\")\n",
        "        print(\"5. customer_metrics_correlation.png\")\n",
        "\n",
        "def main():\n",
        "    # Replace with your actual file path\n",
        "    file_path = \"Online Retail.csv\"\n",
        "\n",
        "    visualizer = RetailDataVisualizer(file_path)\n",
        "    visualizer.generate_advanced_visualizations()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjOqzF1wYYJB",
        "outputId": "c22a74d1-d480-4078-81ff-2a2617fa60c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced Retail Data Visualizations Generated:\n",
            "1. global_sales_distribution.html\n",
            "2. monthly_sales_trend.html\n",
            "3. top_products_sales.html\n",
            "4. customer_segmentation_3d.html\n",
            "5. customer_metrics_correlation.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "# Read the UCI Online Retail dataset\n",
        "def load_and_preprocess_data(filepath):\n",
        "    \"\"\"\n",
        "    Load and preprocess the UCI Online Retail dataset for geographical visualization\n",
        "\n",
        "    Parameters:\n",
        "    filepath (str): Path to the Excel file\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Aggregated sales data by country\n",
        "    \"\"\"\n",
        "    # Read the dataset\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Basic cleaning\n",
        "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "    # Group by country and aggregate sales\n",
        "    country_sales = df.groupby('Country')['TotalSales'].sum().reset_index()\n",
        "\n",
        "    # Log transform sales (adding 1 to handle zero values)\n",
        "    country_sales['LogSales'] = np.log10(country_sales['TotalSales'] + 1)\n",
        "\n",
        "    return country_sales\n",
        "\n",
        "# Create geographical plot\n",
        "def create_geo_plot(country_sales):\n",
        "    \"\"\"\n",
        "    Create a geographical plot of sales data\n",
        "\n",
        "    Parameters:\n",
        "    country_sales (pd.DataFrame): Dataframe with country sales data\n",
        "\n",
        "    Returns:\n",
        "    Plotly Figure object\n",
        "    \"\"\"\n",
        "    fig = px.choropleth(\n",
        "        country_sales,\n",
        "        locations='Country',\n",
        "        locationmode='country names',\n",
        "        color='LogSales',\n",
        "        hover_name='Country',\n",
        "        hover_data=['TotalSales'],\n",
        "        color_continuous_scale='Viridis',\n",
        "        scope='europe',\n",
        "        title='European Online Retail Sales (Log Scaled)'\n",
        "    )\n",
        "\n",
        "    # Customize the layout\n",
        "    fig.update_geos(\n",
        "        showframe=False,\n",
        "        showcoastlines=True,\n",
        "        projection_type='mercator'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Main execution\n",
        "def main(filepath):\n",
        "    \"\"\"\n",
        "    Main function to load data and create visualization\n",
        "\n",
        "    Parameters:\n",
        "    filepath (str): Path to the UCI Online Retail dataset\n",
        "    \"\"\"\n",
        "    # Load and preprocess data\n",
        "    country_sales = load_and_preprocess_data(filepath)\n",
        "\n",
        "    # Create and show the plot\n",
        "    fig = create_geo_plot(country_sales)\n",
        "    fig.show()\n",
        "    fig.write_html('european_retail_sales.html')\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have the UCI Online Retail dataset\n",
        "main('Online Retail.csv')\n",
        "\n",
        "# Note: You'll need to have these libraries installed:\n",
        "# pip install pandas plotly openpyxl numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "msIO4DcacVQo",
        "outputId": "da32a929-e34e-4f22-e91d-f625a3637ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f6532c26-d045-4edb-a846-f87c1beb7784\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6532c26-d045-4edb-a846-f87c1beb7784\")) {                    Plotly.newPlot(                        \"f6532c26-d045-4edb-a846-f87c1beb7784\",                        [{\"coloraxis\":\"coloraxis\",\"customdata\":[[137077.27],[10154.32],[548.4],[40910.96],[1143.6],[3666.38],[20086.29],[12946.29],[707.72],[18768.14],[250285.22],[1291.75],[22326.74],[196712.84],[221698.21],[4710.52],[4310.0],[6994.25],[16890.51],[35340.62],[1693.88],[1661.06],[2505.47],[284661.54],[35163.46],[7213.14],[29059.81],[1002.3100000000001],[131.17],[9120.39],[54774.58],[36595.91],[55739.4],[1730.92],[1902.28],[6767873.394],[2667.07]],\"geo\":\"geo\",\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eCountry=%{location}\\u003cbr\\u003eTotalSales=%{customdata[0]}\\u003cbr\\u003eLogSales=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"Australia\",\"Austria\",\"Bahrain\",\"Belgium\",\"Brazil\",\"Canada\",\"Channel Islands\",\"Cyprus\",\"Czech Republic\",\"Denmark\",\"EIRE\",\"European Community\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Iceland\",\"Israel\",\"Italy\",\"Japan\",\"Lebanon\",\"Lithuania\",\"Malta\",\"Netherlands\",\"Norway\",\"Poland\",\"Portugal\",\"RSA\",\"Saudi Arabia\",\"Singapore\",\"Spain\",\"Sweden\",\"Switzerland\",\"USA\",\"United Arab Emirates\",\"United Kingdom\",\"Unspecified\"],\"locationmode\":\"country names\",\"locations\":[\"Australia\",\"Austria\",\"Bahrain\",\"Belgium\",\"Brazil\",\"Canada\",\"Channel Islands\",\"Cyprus\",\"Czech Republic\",\"Denmark\",\"EIRE\",\"European Community\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Iceland\",\"Israel\",\"Italy\",\"Japan\",\"Lebanon\",\"Lithuania\",\"Malta\",\"Netherlands\",\"Norway\",\"Poland\",\"Portugal\",\"RSA\",\"Saudi Arabia\",\"Singapore\",\"Spain\",\"Sweden\",\"Switzerland\",\"USA\",\"United Arab Emirates\",\"United Kingdom\",\"Unspecified\"],\"name\":\"\",\"z\":[5.136968614767205,4.006693612826459,2.739888655084543,4.611850286068754,3.0586537415723702,3.5643559122384425,4.302921349519108,4.112178875659747,2.8504746886766843,4.2734443737502215,5.398436939329741,3.1115145464441074,4.3488447662778364,5.293834916219197,5.345764145558097,3.6731610390010827,3.634578022853888,3.844803240154554,4.227668474636443,4.548286452950471,3.2291389549386844,3.220646697666245,3.3990625108990313,5.454330320179346,4.546103952594843,3.8581845661272705,4.463307715042106,3.0014351408850666,2.121132889998409,3.9600610250971893,4.738586984853755,4.563444418000941,4.74617008093935,3.238527827426727,3.2795026839831607,6.830452289969074,3.426197219627919],\"type\":\"choropleth\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"geo\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"center\":{},\"scope\":\"europe\",\"projection\":{\"type\":\"mercator\"},\"showframe\":false,\"showcoastlines\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"LogSales\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"European Online Retail Sales (Log Scaled)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f6532c26-d045-4edb-a846-f87c1beb7784');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_data(filepath):\n",
        "    \"\"\"\n",
        "    Comprehensive data loading and preprocessing\n",
        "    \"\"\"\n",
        "    # Read the dataset\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Clean and transform data\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_enhanced_sales_intensity_heatmap(df):\n",
        "    \"\"\"\n",
        "    Create a more detailed sales intensity heatmap with refined time buckets\n",
        "    \"\"\"\n",
        "    # Create more granular time buckets\n",
        "    def create_time_bucket(hour):\n",
        "        if 0 <= hour < 3:\n",
        "            return '00:00-03:00 (Late Night)'\n",
        "        elif 3 <= hour < 6:\n",
        "            return '03:00-06:00 (Early Morning)'\n",
        "        elif 6 <= hour < 9:\n",
        "            return '06:00-09:00 (Morning)'\n",
        "        elif 9 <= hour < 12:\n",
        "            return '09:00-12:00 (Late Morning)'\n",
        "        elif 12 <= hour < 15:\n",
        "            return '12:00-15:00 (Early Afternoon)'\n",
        "        elif 15 <= hour < 18:\n",
        "            return '15:00-18:00 (Late Afternoon)'\n",
        "        elif 18 <= hour < 21:\n",
        "            return '18:00-21:00 (Evening)'\n",
        "        else:\n",
        "            return '21:00-00:00 (Night)'\n",
        "\n",
        "    # Extract hour and prepare data\n",
        "    df['Hour'] = df['InvoiceDate'].dt.hour\n",
        "    df['TimeBucket'] = df['Hour'].apply(create_time_bucket)\n",
        "    df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()\n",
        "\n",
        "    # Define day order\n",
        "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "    # Time bucket order\n",
        "    time_bucket_order = [\n",
        "        '00:00-03:00 (Late Night)',\n",
        "        '03:00-06:00 (Early Morning)',\n",
        "        '06:00-09:00 (Morning)',\n",
        "        '09:00-12:00 (Late Morning)',\n",
        "        '12:00-15:00 (Early Afternoon)',\n",
        "        '15:00-18:00 (Late Afternoon)',\n",
        "        '18:00-21:00 (Evening)',\n",
        "        '21:00-00:00 (Night)'\n",
        "    ]\n",
        "\n",
        "    # Aggregate sales by day and time bucket\n",
        "    hourly_sales = df.groupby(['DayOfWeek', 'TimeBucket'])['TotalSales'].agg([\n",
        "        'sum',      # Total sales\n",
        "        'count',    # Number of transactions\n",
        "        'mean'      # Average transaction value\n",
        "    ]).reset_index()\n",
        "\n",
        "    # Prepare categorical data\n",
        "    hourly_sales['DayOfWeek'] = pd.Categorical(hourly_sales['DayOfWeek'], categories=day_order, ordered=True)\n",
        "    hourly_sales['TimeBucket'] = pd.Categorical(hourly_sales['TimeBucket'], categories=time_bucket_order, ordered=True)\n",
        "    hourly_sales = hourly_sales.sort_values(['DayOfWeek', 'TimeBucket'])\n",
        "\n",
        "    # Create multiple visualizations\n",
        "\n",
        "    # 1. Sales Total Heatmap\n",
        "    fig_total = px.density_heatmap(\n",
        "        hourly_sales,\n",
        "        x='TimeBucket',\n",
        "        y='DayOfWeek',\n",
        "        z='sum',\n",
        "        title='Sales Intensity Heatmap: Total Sales',\n",
        "        labels={'sum': 'Total Sales (£)'},\n",
        "        color_continuous_scale='magma',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    fig_total.update_xaxes(tickangle=45)\n",
        "    fig_total.write_html('sales_intensity_total.html')\n",
        "\n",
        "    # 2. Transaction Count Heatmap\n",
        "    fig_count = px.density_heatmap(\n",
        "        hourly_sales,\n",
        "        x='TimeBucket',\n",
        "        y='DayOfWeek',\n",
        "        z='count',\n",
        "        title='Sales Intensity Heatmap: Number of Transactions',\n",
        "        labels={'count': 'Number of Transactions'},\n",
        "        color_continuous_scale='viridis',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    fig_count.update_xaxes(tickangle=45)\n",
        "    fig_count.write_html('sales_intensity_count.html')\n",
        "\n",
        "    # 3. Average Transaction Value Heatmap\n",
        "    fig_avg = px.density_heatmap(\n",
        "        hourly_sales,\n",
        "        x='TimeBucket',\n",
        "        y='DayOfWeek',\n",
        "        z='mean',\n",
        "        title='Sales Intensity Heatmap: Average Transaction Value',\n",
        "        labels={'mean': 'Average Transaction Value (£)'},\n",
        "        color_continuous_scale='plasma',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    fig_avg.update_xaxes(tickangle=45)\n",
        "    fig_avg.write_html('sales_intensity_avg.html')\n",
        "\n",
        "    # 4. Interactive Bubble Plot for Comprehensive View\n",
        "    fig_bubble = go.Figure(data=[go.Scatter(\n",
        "        x=hourly_sales['TimeBucket'],\n",
        "        y=hourly_sales['DayOfWeek'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=hourly_sales['count'] / 50,  # Size based on transaction count\n",
        "            color=hourly_sales['sum'],  # Color based on total sales\n",
        "            colorscale='Viridis',\n",
        "            showscale=True,\n",
        "            colorbar=dict(title='Total Sales (£)')\n",
        "        ),\n",
        "        text=hourly_sales.apply(lambda row:\n",
        "            f\"Day: {row['DayOfWeek']}<br>\" +\n",
        "            f\"Time: {row['TimeBucket']}<br>\" +\n",
        "            f\"Total Sales: £{row['sum']:,.2f}<br>\" +\n",
        "            f\"Transactions: {row['count']}<br>\" +\n",
        "            f\"Avg Transaction: £{row['mean']:,.2f}\", axis=1),\n",
        "        hoverinfo='text'\n",
        "    )])\n",
        "\n",
        "    fig_bubble.update_layout(\n",
        "        title='Comprehensive Sales Intensity Visualization',\n",
        "        xaxis_title='Time Bucket',\n",
        "        yaxis_title='Day of Week',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    fig_bubble.update_xaxes(tickangle=45)\n",
        "    fig_bubble.write_html('sales_intensity_bubble.html')\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"Sales Intensity Analysis Summary:\")\n",
        "    print(hourly_sales.groupby('TimeBucket')[['sum', 'count', 'mean']].sum())\n",
        "\n",
        "# Main execution\n",
        "def main(filepath):\n",
        "    \"\"\"\n",
        "    Main function to load data and create visualizations\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = load_and_preprocess_data(filepath)\n",
        "\n",
        "    # Create enhanced sales intensity visualization\n",
        "    create_enhanced_sales_intensity_heatmap(df)\n",
        "\n",
        "# Example usage\n",
        "main('Online Retail.csv')\n",
        "\n",
        "# Note: Requires these libraries\n",
        "# pip install pandas plotly openpyxl numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIeqWFZSdjq4",
        "outputId": "c072f80e-81fc-489a-d5a3-c7467b023208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Intensity Analysis Summary:\n",
            "                                       sum   count        mean\n",
            "TimeBucket                                                    \n",
            "00:00-03:00 (Late Night)             0.000       0    0.000000\n",
            "03:00-06:00 (Early Morning)          0.000       0    0.000000\n",
            "06:00-09:00 (Morning)           310615.240    9216  167.821549\n",
            "09:00-12:00 (Late Morning)     2860310.702  111664  150.577283\n",
            "12:00-15:00 (Early Afternoon)  3408371.021  193479  106.712981\n",
            "15:00-18:00 (Late Afternoon)   1559325.481   84951  108.648023\n",
            "18:00-21:00 (Evening)           161443.370    7519  484.786041\n",
            "21:00-00:00 (Night)                  0.000       0    0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-417bcfc54814>:154: FutureWarning:\n",
            "\n",
            "The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RetailDataAnalyzer:\n",
        "    def __init__(self, filepath: str):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with performance tracking and logging.\n",
        "\n",
        "        :param filepath: Path to the retail dataset CSV\n",
        "        \"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        logger.info(f\"Retail Data Analyzer initialized with file: {filepath}\")\n",
        "\n",
        "    def load_and_preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Enhanced data loading with comprehensive preprocessing and performance logging.\n",
        "\n",
        "        :return: Preprocessed pandas DataFrame\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting data loading and preprocessing...\")\n",
        "        preprocessing_start = time.time()\n",
        "\n",
        "        # Read with optimized parameters\n",
        "        df = pd.read_csv(\n",
        "            self.filepath,\n",
        "            parse_dates=['InvoiceDate'],  # Built-in datetime parsing\n",
        "            infer_datetime_format=True,   # Faster datetime inference\n",
        "            low_memory=False              # Handle mixed data types\n",
        "        )\n",
        "\n",
        "        # Vectorized operations for efficiency\n",
        "        df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "        # Efficient datetime extraction\n",
        "        df['Year'] = df['InvoiceDate'].dt.year\n",
        "        df['Month'] = df['InvoiceDate'].dt.month\n",
        "        df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()\n",
        "\n",
        "        # Remove potential duplicates and invalid entries\n",
        "        df.drop_duplicates(inplace=True)\n",
        "        df.dropna(subset=['Description', 'Quantity', 'UnitPrice'], inplace=True)\n",
        "\n",
        "        self.df = df\n",
        "        preprocessing_time = time.time() - preprocessing_start\n",
        "        logger.info(f\"Data preprocessing completed in {preprocessing_time:.2f} seconds\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_parallel_coordinates(self) -> None:\n",
        "        \"\"\"Create parallel coordinates visualization with performance optimization.\"\"\"\n",
        "        logger.info(\"Generating Parallel Coordinates Visualization...\")\n",
        "\n",
        "        product_performance = (\n",
        "            self.df.groupby('Description')\n",
        "            .agg({\n",
        "                'Quantity': ['sum', 'mean'],\n",
        "                'UnitPrice': ['mean', 'max'],\n",
        "                'TotalSales': ['sum', 'mean']\n",
        "            })\n",
        "            .reset_index()\n",
        "        )\n",
        "\n",
        "        # Flatten multi-level columns efficiently\n",
        "        product_performance.columns = [\n",
        "            'Description', 'Total_Quantity', 'Avg_Quantity',\n",
        "            'Avg_Price', 'Max_Price', 'Total_Sales', 'Avg_Sales'\n",
        "        ]\n",
        "\n",
        "        # Normalize with sklearn\n",
        "        scaler = MinMaxScaler()\n",
        "        columns_to_normalize = [\n",
        "            'Total_Quantity', 'Avg_Quantity', 'Avg_Price',\n",
        "            'Max_Price', 'Total_Sales', 'Avg_Sales'\n",
        "        ]\n",
        "        product_performance[columns_to_normalize] = scaler.fit_transform(\n",
        "            product_performance[columns_to_normalize]\n",
        "        )\n",
        "\n",
        "        # Use Plotly's optimized rendering\n",
        "        fig = go.Figure(data=\n",
        "            go.Parcoords(\n",
        "                line=dict(\n",
        "                    color=product_performance['Total_Sales'],\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True\n",
        "                ),\n",
        "                dimensions=[\n",
        "                    dict(range=[0, 1], label=col, values=product_performance[col])\n",
        "                    for col in columns_to_normalize\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Optimized Product Performance Analysis',\n",
        "            height=800, width=1200\n",
        "        )\n",
        "        fig.write_html('parallel_coordinates_performance.html')\n",
        "        logger.info(\"Parallel Coordinates Visualization complete.\")\n",
        "\n",
        "    def create_product_network(self) -> None:\n",
        "        \"\"\"Create an improved product co-purchase network graph.\"\"\"\n",
        "        logger.info(\"Generating Improved Product Network Visualization...\")\n",
        "\n",
        "        # Efficient pivot and co-occurrence calculation\n",
        "        product_invoice = self.df.pivot_table(\n",
        "            index='InvoiceNo',\n",
        "            columns='Description',\n",
        "            aggfunc='size',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        sparse_matrix = csr_matrix(product_invoice.values)\n",
        "\n",
        "        # Perform dot product on sparse matrix\n",
        "        product_pairs = sparse_matrix.T.dot(sparse_matrix)\n",
        "\n",
        "        # Convert back to DataFrame\n",
        "        product_pairs = pd.DataFrame(product_pairs.toarray(),\n",
        "                                    index=product_invoice.columns,\n",
        "                                    columns=product_invoice.columns)\n",
        "        np.fill_diagonal(product_pairs.values, 0)\n",
        "\n",
        "        # Extract top co-purchase relationships\n",
        "        product_pairs.index.name = 'Product_A'\n",
        "        product_pairs.columns.name = None\n",
        "        co_purchase_counts = (\n",
        "            product_pairs.stack()\n",
        "            .reset_index(name='Frequency')\n",
        "            .query(\"Frequency > 0\")\n",
        "        )\n",
        "        co_purchase_counts.columns = ['Product_A', 'Product_B', 'Frequency']\n",
        "        top_co_purchases = co_purchase_counts.nlargest(100, 'Frequency')\n",
        "\n",
        "        # Build the NetworkX graph\n",
        "        G = nx.from_pandas_edgelist(\n",
        "            top_co_purchases,\n",
        "            'Product_A',\n",
        "            'Product_B',\n",
        "            edge_attr='Frequency'\n",
        "        )\n",
        "\n",
        "        # Calculate node centrality to determine size\n",
        "        node_centrality = nx.degree_centrality(G)\n",
        "        node_sizes = [10 + node_centrality[node] * 100 for node in G.nodes()]\n",
        "\n",
        "        # Generate positions with force-directed layout\n",
        "        pos = nx.spring_layout(G, k=0.5, seed=42)\n",
        "\n",
        "        # Create the Plotly figure\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add edges\n",
        "        for edge in G.edges(data=True):\n",
        "            x0, y0 = pos[edge[0]]\n",
        "            x1, y1 = pos[edge[1]]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[x0, x1, None],\n",
        "                y=[y0, y1, None],\n",
        "                mode='lines',\n",
        "                line=dict(width=edge[2]['Frequency']/100, color='gray'),\n",
        "                hoverinfo='none'\n",
        "            ))\n",
        "\n",
        "        # Add nodes\n",
        "        for i, node in enumerate(G.nodes(data=False)):  # Use enumerate to get the index\n",
        "            x, y = pos[node]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[x],\n",
        "                y=[y],\n",
        "                mode='markers+text',\n",
        "                marker=dict(size=node_sizes[i], color='blue', opacity=0.8), # Use i as the index\n",
        "                text=node,\n",
        "                textposition='top center',\n",
        "                hoverinfo='text',\n",
        "                name=node\n",
        "            ))\n",
        "\n",
        "        # Update layout for better aesthetics\n",
        "        fig.update_layout(\n",
        "            title=\"Improved Product Co-purchase Network\",\n",
        "            title_x=0.5,\n",
        "            showlegend=False,\n",
        "            xaxis=dict(showgrid=False, zeroline=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False),\n",
        "            height=800,\n",
        "            width=1200\n",
        "        )\n",
        "\n",
        "        # Save the plot to an HTML file\n",
        "        fig.write_html('improved_product_network.html')\n",
        "        logger.info(\"Improved Product Network Visualization complete.\")\n",
        "\n",
        "\n",
        "    def execute_analysis(self) -> None:\n",
        "        \"\"\"\n",
        "        Execute the full analysis pipeline with performance tracking.\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting comprehensive retail data analysis...\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self.load_and_preprocess_data()\n",
        "\n",
        "        # Generate visualizations\n",
        "        visualization_methods = [\n",
        "            self.create_parallel_coordinates,\n",
        "            self.create_product_network\n",
        "        ]\n",
        "\n",
        "        for method in visualization_methods:\n",
        "            method_start = time.time()\n",
        "            method()\n",
        "            logger.info(f\"{method.__name__} completed in {time.time() - method_start:.2f} seconds\")\n",
        "\n",
        "        total_time = time.time() - self.start_time\n",
        "        logger.info(f\"Total analysis completed in {total_time:.2f} seconds\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    analyzer = RetailDataAnalyzer('Online Retail.csv')\n",
        "    analyzer.execute_analysis()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Performance dependencies:\n",
        "# pip install pandas plotly networkx scikit-learn numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHtrinmVeuOc",
        "outputId": "85395ec2-48ce-44d2-c5a5-f0d3310a2f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-14f93d653ba3>:44: FutureWarning:\n",
            "\n",
            "The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy import stats\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RetailDataAnalyzer:\n",
        "    def __init__(self, filepath: str):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with performance tracking and logging.\n",
        "\n",
        "        :param filepath: Path to the retail dataset CSV\n",
        "        \"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        logger.info(f\"Retail Data Analyzer initialized with file: {filepath}\")\n",
        "\n",
        "    def load_and_preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Enhanced data loading with comprehensive preprocessing and performance logging.\n",
        "\n",
        "        :return: Preprocessed pandas DataFrame\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting data loading and preprocessing...\")\n",
        "        preprocessing_start = time.time()\n",
        "\n",
        "        # Read with optimized parameters\n",
        "        df = pd.read_csv(\n",
        "            self.filepath,\n",
        "            parse_dates=['InvoiceDate'],  # Built-in datetime parsing\n",
        "            infer_datetime_format=True,   # Faster datetime inference\n",
        "            low_memory=False              # Handle mixed data types\n",
        "        )\n",
        "\n",
        "        # Vectorized operations for efficiency\n",
        "        df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "        # Efficient datetime extraction\n",
        "        df['Year'] = df['InvoiceDate'].dt.year\n",
        "        df['Month'] = df['InvoiceDate'].dt.month\n",
        "        df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()\n",
        "\n",
        "        # Remove potential duplicates and invalid entries\n",
        "        df.drop_duplicates(inplace=True)\n",
        "        df.dropna(subset=['Description', 'Quantity', 'UnitPrice'], inplace=True)\n",
        "\n",
        "        self.df = df\n",
        "        preprocessing_time = time.time() - preprocessing_start\n",
        "        logger.info(f\"Data preprocessing completed in {preprocessing_time:.2f} seconds\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_parallel_coordinates(self) -> None:\n",
        "        \"\"\"Create parallel coordinates visualization with performance optimization.\"\"\"\n",
        "        logger.info(\"Generating Parallel Coordinates Visualization...\")\n",
        "\n",
        "        product_performance = (\n",
        "            self.df.groupby('Description')\n",
        "            .agg({\n",
        "                'Quantity': ['sum', 'mean'],\n",
        "                'UnitPrice': ['mean', 'max'],\n",
        "                'TotalSales': ['sum', 'mean']\n",
        "            })\n",
        "            .reset_index()\n",
        "        )\n",
        "\n",
        "        # Flatten multi-level columns efficiently\n",
        "        product_performance.columns = [\n",
        "            'Description', 'Total_Quantity', 'Avg_Quantity',\n",
        "            'Avg_Price', 'Max_Price', 'Total_Sales', 'Avg_Sales'\n",
        "        ]\n",
        "\n",
        "        # Normalize with sklearn\n",
        "        scaler = MinMaxScaler()\n",
        "        columns_to_normalize = [\n",
        "            'Total_Quantity', 'Avg_Quantity', 'Avg_Price',\n",
        "            'Max_Price', 'Total_Sales', 'Avg_Sales'\n",
        "        ]\n",
        "        product_performance[columns_to_normalize] = scaler.fit_transform(\n",
        "            product_performance[columns_to_normalize]\n",
        "        )\n",
        "\n",
        "        # Use Plotly's optimized rendering\n",
        "        fig = go.Figure(data=\n",
        "            go.Parcoords(\n",
        "                line=dict(\n",
        "                    color=product_performance['Total_Sales'],\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True\n",
        "                ),\n",
        "                dimensions=[\n",
        "                    dict(range=[0, 1], label=col, values=product_performance[col])\n",
        "                    for col in columns_to_normalize\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Optimized Product Performance Analysis',\n",
        "            height=800, width=1200\n",
        "        )\n",
        "        fig.write_html('parallel_coordinates_performance.html')\n",
        "        logger.info(\"Parallel Coordinates Visualization complete.\")\n",
        "\n",
        "    def create_product_network(self) -> None:\n",
        "        \"\"\"Create an improved product co-purchase network graph.\"\"\"\n",
        "        logger.info(\"Generating Improved Product Network Visualization...\")\n",
        "\n",
        "        # Efficient pivot and co-occurrence calculation\n",
        "        product_invoice = self.df.pivot_table(\n",
        "            index='InvoiceNo',\n",
        "            columns='Description',\n",
        "            aggfunc='size',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        sparse_matrix = csr_matrix(product_invoice.values)\n",
        "\n",
        "        # Perform dot product on sparse matrix\n",
        "        product_pairs = sparse_matrix.T.dot(sparse_matrix)\n",
        "\n",
        "        # Convert back to DataFrame\n",
        "        product_pairs = pd.DataFrame(product_pairs.toarray(),\n",
        "                                    index=product_invoice.columns,\n",
        "                                    columns=product_invoice.columns)\n",
        "        np.fill_diagonal(product_pairs.values, 0)\n",
        "\n",
        "        # Extract top co-purchase relationships\n",
        "        product_pairs.index.name = 'Product_A'\n",
        "        product_pairs.columns.name = None\n",
        "        co_purchase_counts = (\n",
        "            product_pairs.stack()\n",
        "            .reset_index(name='Frequency')\n",
        "            .query(\"Frequency > 0\")\n",
        "        )\n",
        "        co_purchase_counts.columns = ['Product_A', 'Product_B', 'Frequency']\n",
        "        top_co_purchases = co_purchase_counts.nlargest(100, 'Frequency')\n",
        "\n",
        "        # Build the NetworkX graph\n",
        "        G = nx.from_pandas_edgelist(\n",
        "            top_co_purchases,\n",
        "            'Product_A',\n",
        "            'Product_B',\n",
        "            edge_attr='Frequency'\n",
        "        )\n",
        "\n",
        "        # Calculate node centrality to determine size\n",
        "        node_centrality = nx.degree_centrality(G)\n",
        "        node_sizes = [10 + node_centrality[node] * 100 for node in G.nodes()]\n",
        "\n",
        "        # Generate positions with force-directed layout\n",
        "        pos = nx.spring_layout(G, k=0.5, seed=42)\n",
        "\n",
        "        # Create the Plotly figure\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add edges\n",
        "        for edge in G.edges(data=True):\n",
        "            x0, y0 = pos[edge[0]]\n",
        "            x1, y1 = pos[edge[1]]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[x0, x1, None],\n",
        "                y=[y0, y1, None],\n",
        "                mode='lines',\n",
        "                line=dict(width=edge[2]['Frequency']/100, color='gray'),\n",
        "                hoverinfo='none'\n",
        "            ))\n",
        "\n",
        "        # Add nodes\n",
        "        for i, node in enumerate(G.nodes(data=False)):  # Use enumerate to get the index\n",
        "            x, y = pos[node]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[x],\n",
        "                y=[y],\n",
        "                mode='markers+text',\n",
        "                marker=dict(size=node_sizes[i], color='blue', opacity=0.8), # Use i as the index\n",
        "                text=node,\n",
        "                textposition='top center',\n",
        "                hoverinfo='text',\n",
        "                name=node\n",
        "            ))\n",
        "\n",
        "        # Update layout for better aesthetics\n",
        "        fig.update_layout(\n",
        "            title=\"Improved Product Co-purchase Network\",\n",
        "            title_x=0.5,\n",
        "            showlegend=False,\n",
        "            xaxis=dict(showgrid=False, zeroline=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False),\n",
        "            height=800,\n",
        "            width=1200\n",
        "        )\n",
        "\n",
        "        # Save the plot to an HTML file\n",
        "        fig.write_html('improved_product_network.html')\n",
        "        logger.info(\"Improved Product Network Visualization complete.\")\n",
        "\n",
        "    def generate_advanced_3d_visualizations(self):\n",
        "        \"\"\"\n",
        "        Create advanced 3D visualizations to provide deep insights into retail data.\n",
        "        \"\"\"\n",
        "        logger.info(\"Generating Advanced 3D Visualizations...\")\n",
        "\n",
        "        # 1. 3D Scatter Plot of Sales Performance with Depth and Color Encoding\n",
        "        def create_3d_sales_performance_scatter():\n",
        "            \"\"\"\n",
        "            Create a 3D scatter plot that visualizes product performance across multiple dimensions.\n",
        "            \"\"\"\n",
        "            # Group by product and calculate comprehensive metrics\n",
        "            product_stats = (\n",
        "                self.df.groupby('Description')\n",
        "                .agg({\n",
        "                    'Quantity': ['sum', 'mean'],\n",
        "                    'UnitPrice': ['mean', 'max'],\n",
        "                    'TotalSales': ['sum', 'mean']\n",
        "                })\n",
        "                .reset_index()\n",
        "            )\n",
        "\n",
        "            # Flatten multi-level columns\n",
        "            product_stats.columns = [\n",
        "                'Description', 'Total_Quantity', 'Avg_Quantity',\n",
        "                'Avg_Price', 'Max_Price', 'Total_Sales', 'Avg_Sales'\n",
        "            ]\n",
        "\n",
        "            # Create 3D scatter plot\n",
        "            fig = go.Figure(data=[go.Scatter3d(\n",
        "                x=product_stats['Total_Quantity'],\n",
        "                y=product_stats['Avg_Price'],\n",
        "                z=product_stats['Total_Sales'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=5,\n",
        "                    color=product_stats['Avg_Sales'],  # color by average sales\n",
        "                    colorscale='Viridis',\n",
        "                    opacity=0.8,\n",
        "                    colorbar=dict(title='Avg Sales')\n",
        "                ),\n",
        "                text=product_stats['Description'],\n",
        "                hoverinfo='text'\n",
        "            )])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='3D Product Performance Visualization',\n",
        "                scene=dict(\n",
        "                    xaxis_title='Total Quantity',\n",
        "                    yaxis_title='Average Price',\n",
        "                    zaxis_title='Total Sales'\n",
        "                ),\n",
        "                height=800,\n",
        "                width=1200\n",
        "            )\n",
        "\n",
        "            fig.write_html('3d_sales_performance_scatter.html')\n",
        "            logger.info(\"3D Sales Performance Scatter Plot complete.\")\n",
        "\n",
        "        # 2. 3D Density Estimation of Purchase Patterns\n",
        "        def create_3d_purchase_density():\n",
        "            \"\"\"\n",
        "            Create a 3D density plot to visualize the distribution of purchases.\n",
        "            \"\"\"\n",
        "            # Prepare data for kernel density estimation\n",
        "            daily_sales = (\n",
        "                self.df.groupby(['Year', 'Month', 'DayOfWeek'])['TotalSales']\n",
        "                .sum()\n",
        "                .reset_index()\n",
        "            )\n",
        "\n",
        "            # Map day names to numeric values for 3D plotting\n",
        "            day_map = {day: idx for idx, day in enumerate(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])}\n",
        "            daily_sales['DayNumeric'] = daily_sales['DayOfWeek'].map(day_map)\n",
        "\n",
        "            # Kernel Density Estimation\n",
        "            x = daily_sales['Month']\n",
        "            y = daily_sales['DayNumeric']\n",
        "            z = daily_sales['TotalSales']\n",
        "\n",
        "            # Create a grid of points\n",
        "            xi = np.linspace(x.min(), x.max(), 100)\n",
        "            yi = np.linspace(y.min(), y.max(), 100)\n",
        "            xi, yi = np.meshgrid(xi, yi)\n",
        "\n",
        "            # Perform kernel density estimation\n",
        "            kernel = stats.gaussian_kde(np.vstack([x, y, z]))\n",
        "            zi = kernel(np.vstack([xi.ravel(), yi.ravel(), z.mean() * np.ones_like(xi.ravel())]))\n",
        "\n",
        "            # Create 3D surface plot\n",
        "            fig = go.Figure(data=[\n",
        "                go.Surface(\n",
        "                    x=xi,\n",
        "                    y=yi,\n",
        "                    z=zi.reshape(xi.shape),\n",
        "                    colorscale='Plasma',\n",
        "                    colorbar=dict(title='Sales Density')\n",
        "                )\n",
        "            ])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='3D Sales Density Estimation',\n",
        "                scene=dict(\n",
        "                    xaxis_title='Month',\n",
        "                    yaxis_title='Day of Week',\n",
        "                    zaxis_title='Sales Density'\n",
        "                ),\n",
        "                height=800,\n",
        "                width=1200\n",
        "            )\n",
        "\n",
        "            fig.write_html('3d_purchase_density.html')\n",
        "            logger.info(\"3D Purchase Density Plot complete.\")\n",
        "\n",
        "        # 3. 3D Surface Plot of Seasonal Sales Trends\n",
        "        def create_seasonal_sales_surface():\n",
        "            \"\"\"\n",
        "            Generate a 3D surface plot showing seasonal sales trends across different product categories.\n",
        "            \"\"\"\n",
        "            # Aggregate sales by month, year, and product category\n",
        "            seasonal_sales = (\n",
        "                self.df.groupby([\n",
        "                    pd.Grouper(key='InvoiceDate', freq='M'),\n",
        "                    'Description'\n",
        "                ])['TotalSales']\n",
        "                .sum()\n",
        "                .unstack(fill_value=0)\n",
        "            )\n",
        "\n",
        "            # Select top categories\n",
        "            top_categories = seasonal_sales.sum().nlargest(10).index\n",
        "            seasonal_subset = seasonal_sales[top_categories]\n",
        "\n",
        "            # Prepare data for surface plot\n",
        "            months = seasonal_subset.index.month\n",
        "            years = seasonal_subset.index.year\n",
        "\n",
        "            # Create mesh grid\n",
        "            unique_months = np.unique(months)\n",
        "            unique_years = np.unique(years)\n",
        "\n",
        "            # Initialize surface data\n",
        "            surface_data = np.zeros((len(top_categories), len(unique_years), len(unique_months)))\n",
        "\n",
        "            for i, category in enumerate(top_categories):\n",
        "                for j, year in enumerate(unique_years):\n",
        "                    for k, month in enumerate(unique_months):\n",
        "                        mask = (months == month) & (years == year)\n",
        "                        surface_data[i, j, k] = seasonal_subset.loc[mask, category].sum()\n",
        "\n",
        "            # Create 3D surface plot\n",
        "            fig = go.Figure()\n",
        "            for i, category in enumerate(top_categories):\n",
        "                fig.add_trace(go.Surface(\n",
        "                    z=surface_data[i],\n",
        "                    x=unique_years,\n",
        "                    y=unique_months,\n",
        "                    name=category,\n",
        "                    colorscale='Jet',\n",
        "                    showscale=False\n",
        "                ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='3D Seasonal Sales Trends by Top Categories',\n",
        "                scene=dict(\n",
        "                    xaxis_title='Year',\n",
        "                    yaxis_title='Month',\n",
        "                    zaxis_title='Total Sales'\n",
        "                ),\n",
        "                height=800,\n",
        "                width=1200\n",
        "            )\n",
        "\n",
        "            fig.write_html('3d_seasonal_sales_surface.html')\n",
        "            logger.info(\"3D Seasonal Sales Surface Plot complete.\")\n",
        "\n",
        "        # Execute all 3D visualization methods\n",
        "        visualization_methods = [\n",
        "            create_3d_sales_performance_scatter,\n",
        "            create_3d_purchase_density,\n",
        "            create_seasonal_sales_surface\n",
        "        ]\n",
        "\n",
        "        for method in visualization_methods:\n",
        "            method_start = time.time()\n",
        "            method()\n",
        "            logger.info(f\"{method.__name__} completed in {time.time() - method_start:.2f} seconds\")\n",
        "\n",
        "    def execute_analysis(self) -> None:\n",
        "        \"\"\"\n",
        "        Execute the full analysis pipeline with performance tracking.\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting comprehensive retail data analysis...\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self.load_and_preprocess_data()\n",
        "\n",
        "        # Generate visualizations\n",
        "        visualization_methods = [\n",
        "            self.create_parallel_coordinates,\n",
        "            self.create_product_network,\n",
        "            self.generate_advanced_3d_visualizations\n",
        "        ]\n",
        "\n",
        "        for method in visualization_methods:\n",
        "            method_start = time.time()\n",
        "            method()\n",
        "            logger.info(f\"{method.__name__} completed in {time.time() - method_start:.2f} seconds\")\n",
        "\n",
        "# analyzer = RetailDataAnalyzer('Online Retail.csv')\n",
        "# analyzer.execute_analysis()"
      ],
      "metadata": {
        "id": "sA3eBvC7gt1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7cc2Fhw1BnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}